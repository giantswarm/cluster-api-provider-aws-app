name: cluster-api-provider-aws
# This refers to a release in the Giant Swarm fork. The fork closely follows upstream releases unless hotfixes are
# needed. Please read https://github.com/giantswarm/cluster-api-provider-aws/blob/main/README.md on how to create a
# release. Please include the short commit SHA in the tag name, such as `v2.0.2-gs-123abcd`. After changing this
# tag, please run `make generate` to update CRDs and other manifests.

# Changes in this tag include upstream v2.9.x with additional:
#   * Giant Swarm fork modifications (https://github.com/giantswarm/cluster-api-provider-aws/pull/641)
#   * Only manage security groups for ENIs tagged by CAPA (https://github.com/giantswarm/cluster-api-provider-aws/pull/616)
#   * Add karpenter IAM Roles to the S3 bucket policy so that karpenter nodes can read userdata (https://github.com/giantswarm/cluster-api-provider-aws/pull/618)
#   * Deflake ROSA test (https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5763; partially cherry-picked; just to keep our fork builds stable)
#   * Cancel instance refresh on any relevant change to ASG instead of blocking until previous one is finished (https://github.com/giantswarm/cluster-api-provider-aws/pull/598 / https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5543)
#   * Check control plane version skew before creating new launch template version (https://github.com/giantswarm/cluster-api-provider-aws/pull/624)
#   * Only try to delete AWSMachine bootstrap data for non-machine pool machines (https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5761)
#   * Fix cluster upgrade version skew (https://github.com/giantswarm/cluster-api-provider-aws/pull/631)
#   *   + extra commit 'Conditionally proceed if `isMachinePoolAllowedToUpgradeDueToControlPlaneVersionSkew` returns an error'
#   * Giant Swarm specific, temporary hack: allow fine-grained control which workload clusters get reconciled with the MachinePool Machines feature (https://github.com/giantswarm/cluster-api-provider-aws/pull/633)
#   * Add update permission for `AWSMachinePool` finalizers as needed by `OwnerReferencesPermissionEnforcement` for setting `BlockOwnerDeletion: true` on AWSMachinePool Machines (https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5722)
#   * Add update permission for `AWSManagedControlPlanes` finalizers as needed by `OwnerReferencesPermissionEnforcement` for setting `BlockOwnerDeletion: true` on EKS kubeconfig Secrets (https://github.com/giantswarm/cluster-api-provider-aws/pull/636)
#   * Fix lifecycle hooks being updated constantly, consider changes to `RoleARN` field as well (https://github.com/kubernetes-sigs/cluster-api-provider-aws/pull/5762)
#   * Cache architecture for each instance type in memory (https://github.com/giantswarm/cluster-api-provider-aws/pull/638)
#   * Remove `TestROSARoleConfigReconcileExist` since it's still flaky and not relevant to our fork (https://github.com/giantswarm/cluster-api-provider-aws/pull/644)
#   * Print reason for launch template needing update, fix constant instance refreshes for AWSMachinePool by considering SSHKeyName nil and empty string the same (https://github.com/giantswarm/cluster-api-provider-aws/pull/643)
#   * Fix constant re-applying of tags if `AWSMachine.metadata.annotations` is nil (https://github.com/giantswarm/cluster-api-provider-aws/pull/646)
tag: v2.9.2-gs-ce25ac670

registry:
  domain: gsoci.azurecr.io

image:
  name: giantswarm/cluster-api-aws-controller

ciliumNetworkPolicy:
  enabled: false

crdConvertOnly: true
crdInstall:
  kubectl:
    image: "giantswarm/kubectl"
    tag: "1.24.10"

capaFeatureGates:
  machinePoolMachines: false
  # Giant Swarm specific to introduce the `MachinePoolMachines` feature gate gradually. This is a regex. If not empty and matching the cluster name, our CAPA fork reconciles it with the feature gate turned on (requiring `capaFeatureGates.machinePoolMachines=true`). This and the corresponding fork code will be removed once all MCs have the feature gate turned on.
  #
  # IMPORTANT: If this regex is left empty, the previous default behavior applies. That means
  #            if `capaFeatureGates.machinePoolMachines=true`, the feature gate is enabled
  #            for *all* workload clusters (and also the MC itself)!
  #            As first migration step, it's recommended to first configure
  #            `capaFeatureGates.machinePoolMachinesOnlyForClusterNameRegex=^(NOT-ON-ANY-CLUSTER-YET)$`
  #            and check the logs. The line `MachinePool Machines feature gate check for cluster <namespace>/<name>: allow=false`
  #            means that a cluster won't be reconciled with the feature, which is a good
  #            starting point. Then, you can set `capaFeatureGates.machinePoolMachines=true`
  #            and a regex which actually selects some or all clusters (don't forget the MC's name,
  #            in addition to WC names).
  machinePoolMachinesOnlyForClusterNameRegex: ""

aws:
  arn: defaultARN

watchfilter: capi

serviceType: managed

provider:
  region: defaultRegion
  capi:
    accessKeyID: defaultID
    secretAccessKey: defaultKey

serviceMonitor:
  enabled: true

verticalPodAutoscaler:
  enabled: true

global:
  podSecurityStandards:
    enforced: false
